<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-remember/remember" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.1.1">
<title data-rh="true">Retrieval-augmented Memory for Embodied Robots | Final Project Team 2</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://ucsd-ecemae-148.github.io/fall-2024-final-project-team-2/img/logo-small.png"><meta data-rh="true" name="twitter:image" content="https://ucsd-ecemae-148.github.io/fall-2024-final-project-team-2/img/logo-small.png"><meta data-rh="true" property="og:url" content="https://ucsd-ecemae-148.github.io/fall-2024-final-project-team-2/docs/remember"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="backend,developer,engineer,grpc,rest,docker,kubernetes,devops,open-source"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Retrieval-augmented Memory for Embodied Robots | Final Project Team 2"><meta data-rh="true" name="description" content="Preface"><meta data-rh="true" property="og:description" content="Preface"><link data-rh="true" rel="icon" href="/fall-2024-final-project-team-2/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://ucsd-ecemae-148.github.io/fall-2024-final-project-team-2/docs/remember"><link data-rh="true" rel="alternate" href="https://ucsd-ecemae-148.github.io/fall-2024-final-project-team-2/docs/remember" hreflang="en"><link data-rh="true" rel="alternate" href="https://ucsd-ecemae-148.github.io/fall-2024-final-project-team-2/docs/remember" hreflang="x-default"><link rel="stylesheet" href="/fall-2024-final-project-team-2/assets/css/styles.f0cd3457.css">
<script src="/fall-2024-final-project-team-2/assets/js/runtime~main.dcff5d84.js" defer="defer"></script>
<script src="/fall-2024-final-project-team-2/assets/js/main.86eacb09.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();null!==e?t(e):window.matchMedia("(prefers-color-scheme: dark)").matches?t("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,t("light"))}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_gu5v" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/fall-2024-final-project-team-2/"><div class="navbar__logo"><img src="/fall-2024-final-project-team-2/img/logo.jpg" alt="El Cochito" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/fall-2024-final-project-team-2/img/logo.jpg" alt="El Cochito" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Team 2, El Cochito</b></a><a class="navbar__item navbar__link" href="/fall-2024-final-project-team-2/projects">Projects</a><a class="navbar__item navbar__link" href="/fall-2024-final-project-team-2/docs/details">Details</a><a class="navbar__item navbar__link" href="/fall-2024-final-project-team-2/thanks">Special Thanks</a></div><div class="navbar__items navbar__items--right"><div class="toggle_kWbt colorModeToggle_GwZs"><button class="clean-btn toggleButton_fOL9 toggleButtonDisabled_STpu" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_DCeJ"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_DFgp"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_IP3a"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_IbdI"><div class="docsWrapper_JGIH"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_SdI4" type="button"></button><div class="docRoot_eRbX"><aside class="theme-doc-sidebar-container docSidebarContainer_Ta75"><div class="sidebarViewport_fgog"><div class="sidebar_oDHW"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_vPEQ"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/fall-2024-final-project-team-2/docs/details">General Overview</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/fall-2024-final-project-team-2/docs/setup">Mechanics and Electronics</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/fall-2024-final-project-team-2/docs/deep-learning">Deep Learning with DonkeyCar</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/fall-2024-final-project-team-2/docs/gps">GPS with PointOneNav and DonkeyCar</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/fall-2024-final-project-team-2/docs/lane-tracking">Lane Tracking with ROS2 and OpenCV2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/fall-2024-final-project-team-2/docs/roboflow">Roboflow with OAKD-Lite</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/fall-2024-final-project-team-2/docs/remember">Retrieval-augmented Memory for Embodied Robots</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/fall-2024-final-project-team-2/docs/human-pose">Human Pose Estimation</a></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_Cq4q"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_eHqP"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_lg0V"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_nDJs"><div class="docItemContainer_OGiL"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_k3Z9" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/fall-2024-final-project-team-2/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_JACu"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Retrieval-augmented Memory for Embodied Robots</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_QCOD theme-doc-toc-mobile tocMobile_N0YI"><button type="button" class="clean-btn tocCollapsibleButton_pHwF">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Retrieval-augmented Memory for Embodied Robots (ReMEmbR)</h1>
<h2 class="anchor anchorWithStickyNavbar_FNw8" id="preface">Preface<a href="#preface" class="hash-link" aria-label="Direct link to Preface" title="Direct link to Preface">​</a></h2>
<p>The goal of the final project was to replicate a <a href="https://nvidia-ai-iot.github.io/remembr/" target="_blank" rel="noopener noreferrer">research paper</a> that allowed robots to establish memory and reason through the memory. It could go back to a given position it has already been to with a prompt.
For example, if asked where the nearest restroom was, it would be able to path plan to the area where it has seen a restroom.</p>
<iframe width="960" height="520" src="https://nvidia-ai-iot.github.io/remembr/static/video/demo.mp4" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" title="Physical DonkeyCar"></iframe>
<div class="theme-admonition theme-admonition-note admonition_LMjb alert alert--secondary"><div class="admonitionHeading_GGQ4"><span class="admonitionIcon_ifdW"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_pGk6"><p>This video is taken from the original research paper.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_FNw8" id="challenges">Challenges<a href="#challenges" class="hash-link" aria-label="Direct link to Challenges" title="Direct link to Challenges">​</a></h2>
<p>The paper used the <a href="https://robotics.segway.com/nova-carter/" target="_blank" rel="noopener noreferrer">Segway Nova Carter</a> which is built on top of the Jetson AGX Orin.
This is an immense difference with our Jetson Nano, almost like comparing a jet to a bicycle. As such, our goal was to replicate this on a constrained and limited hardware platform.</p>
<p>Below is a note of the difference in capability.</p>
<div class="tabs-container tabList_fbd4"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_v5XY tabs__item--active">CPU</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_v5XY">GPU</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_v5XY">RAM</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_v5XY">Storage</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_v5XY">I/O</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem__kUE"><table><thead><tr><th>Aspect</th><th>Jetson AGX Orin</th><th>Jetson Nano</th></tr></thead><tbody><tr><td>Processor</td><td>12-core ARM Cortex-A78AE</td><td>Quad-core ARM Cortex-A57</td></tr><tr><td>Clock Speed</td><td>~2.2 GHz</td><td>~1.43 GHz</td></tr></tbody></table></div><div role="tabpanel" class="tabItem__kUE" hidden=""><table><thead><tr><th>Aspect</th><th>Jetson AGX Orin</th><th>Jetson Nano</th></tr></thead><tbody><tr><td>Architecture</td><td>NVIDIA Ampere</td><td>NVIDIA Maxwell</td></tr><tr><td>CUDA Cores</td><td>2048 CUDA cores</td><td>128 CUDA cores</td></tr><tr><td>Tensor Cores</td><td>64 Tensor Cores</td><td>None</td></tr><tr><td>Performance (Trillions of Operations Per Second)</td><td>275 TOPS</td><td>~0.5 TOPS</td></tr></tbody></table></div><div role="tabpanel" class="tabItem__kUE" hidden=""><table><thead><tr><th>Aspect</th><th>Jetson AGX Orin</th><th>Jetson Nano</th></tr></thead><tbody><tr><td>Type</td><td>64GB LPDDR5</td><td>4GB LPDDR4</td></tr><tr><td>Speed</td><td>High bandwidth (256-bit)</td><td>Lower bandwidth (64-bit)</td></tr></tbody></table></div><div role="tabpanel" class="tabItem__kUE" hidden=""><table><thead><tr><th>Aspect</th><th>Jetson AGX Orin</th><th>Jetson Nano</th></tr></thead><tbody><tr><td>Default Storage</td><td>2TB NVMe SSD</td><td>microSD card</td></tr><tr><td>Interface</td><td>PCIe Gen 4</td><td>microSD interface</td></tr><tr><td>Speed</td><td>High-speed NVMe</td><td>Slower microSD</td></tr></tbody></table></div><div role="tabpanel" class="tabItem__kUE" hidden=""><table><thead><tr><th>Aspect</th><th>Jetson AGX Orin</th><th>Jetson Nano</th></tr></thead><tbody><tr><td>Networking</td><td>10GbE PCIe (optional)</td><td>1GbE Ethernet</td></tr><tr><td>USB</td><td>USB 3.1, USB 3.2 Gen 2</td><td>USB 3.0</td></tr><tr><td>Camera Interfaces</td><td>16x MIPI CSI</td><td>2x MIPI CSI</td></tr><tr><td>GPIO</td><td>Rich GPIO, AI-optimized</td><td>Standard GPIO</td></tr></tbody></table></div></div></div>
<p>It&#x27;s also important to note that the Jetson Nano is limited to Ubuntu 18.04. With some tweaking, it can be flashed to Ubuntu 20.04, but the underlying driver software that supports the GPU is limited to 10.x or Jetpack 4.x. As such, without even considering hardware limitations, our software can only rely on outdated unoptimized GPU libraries.</p>
<p>The Jetson AGX Orin on the other hand defaults on Ubuntu 20.04 and uses the Jetpack 5.x as the base minimum. This means that out of the box, Jetson AGX Orin already has all the optimized software libraries for GPU-acceleration.</p>
<h2 class="anchor anchorWithStickyNavbar_FNw8" id="deliverables">Deliverables<a href="#deliverables" class="hash-link" aria-label="Direct link to Deliverables" title="Direct link to Deliverables">​</a></h2>
<p>The bare minimum for this paper is to replicate the behavior, at least to some degree. This meant that we had to implement the two core functionalities, which were memory building and query. Below is an image that represents the two phases.</p>
<div style="display:flex;justify-content:center;align-items:center"><img src="https://i.gyazo.com/3a152edff226769acb253fab8e4d1610.png" alt="Img"></div>
<p>This means we can partition the deliverable into two.</p>
<h3 class="anchor anchorWithStickyNavbar_FNw8" id="memory-building">Memory Building<a href="#memory-building" class="hash-link" aria-label="Direct link to Memory Building" title="Direct link to Memory Building">​</a></h3>
<p>The underlying technology for building memory consists of three core tools.</p>
<ul>
<li>3D/2D SLAM Mapping</li>
<li>Vision Language model</li>
<li>Vector Database</li>
<li>Retrieval Augmented Generation</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_FNw8" id="slam">SLAM<a href="#slam" class="hash-link" aria-label="Direct link to SLAM" title="Direct link to SLAM">​</a></h4>
<p>SLAM is simultaneous localization and mapping, which maps the environment around us using LIDAR (light).</p>
<h4 class="anchor anchorWithStickyNavbar_FNw8" id="vision-language-model">Vision Language Model<a href="#vision-language-model" class="hash-link" aria-label="Direct link to Vision Language Model" title="Direct link to Vision Language Model">​</a></h4>
<p>A Vision Language Model is a deep learning computer vision model that generates additional information alongside the captured scenery. This means that instead of the conventional object detection or segmentation models, each image or a batch of images can have captions which provides natural language information.</p>
<h4 class="anchor anchorWithStickyNavbar_FNw8" id="vector-database">Vector Database<a href="#vector-database" class="hash-link" aria-label="Direct link to Vector Database" title="Direct link to Vector Database">​</a></h4>
<p>A vector database is simply a database that uses the inherent properties of vectors to store data. A vector is an element that belongs to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>R</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">R^{n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span></span></span></span></span>. This is no different than an image that is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="double-struck">R</mi><mrow><mi>n</mi><mo>×</mo><mi>m</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbb{R}^{n\times m}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7713em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">m</span></span></span></span></span></span></span></span></span></span></span></span> and we partition the image into <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">m</span></span></span></span> different column vectors of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>R</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">R^{n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span></span></span></span></span>. This is useful because we can now apply mathematics for similarity. Before we do so though, we have to encode any non-numerical information into numerics. This can be done by any method.</p>
<h4 class="anchor anchorWithStickyNavbar_FNw8" id="retrieval-augmented-generation">Retrieval Augmented Generation<a href="#retrieval-augmented-generation" class="hash-link" aria-label="Direct link to Retrieval Augmented Generation" title="Direct link to Retrieval Augmented Generation">​</a></h4>
<p>Retrieval Augmented Generation allows us to find which vectors are the most similar so that we can deliver the one that&#x27;s relevant. In this context, given one vector representing a command, we can apply RAG techniques such as cosine-similarity to see which vector within our memory database is the closest. Since we have caption information for our vectors, both query and memory vectors live in the same embedding space.</p>
<h4 class="anchor anchorWithStickyNavbar_FNw8" id="implementation-challenges">Implementation Challenges<a href="#implementation-challenges" class="hash-link" aria-label="Direct link to Implementation Challenges" title="Direct link to Implementation Challenges">​</a></h4>
<h4 class="anchor anchorWithStickyNavbar_FNw8" id="slam-1">SLAM<a href="#slam-1" class="hash-link" aria-label="Direct link to SLAM" title="Direct link to SLAM">​</a></h4>
<p>The original paper used 3D LIDAR, but since we had an OAKD-Lite that provided a variety of features such as depth and distance to said object, we only needed 2D LIDAR. Unfortunately, we could not set up SLAM because of environmental incompatibility. Since our Jetson Nano supported Ubuntu 18.04, and the ROS2 framework for DepthAI required Ubuntu versions 20.04 and newer, we had to resort to not using it. This meant that we had to build our own internal boilerplate for figuring out the height.</p>
<h5 class="anchor anchorWithStickyNavbar_FNw8" id="vision-language-model-1">Vision Language Model<a href="#vision-language-model-1" class="hash-link" aria-label="Direct link to Vision Language Model" title="Direct link to Vision Language Model">​</a></h5>
<p>Since we knew the limitations of our hardware, we had to choose another VLM other than the one used in the research paper which was VILA. We first decided to create an MVP to see if it was possible on our personal computer, and saw that it indeed was possible.
Below is us utilizing BLIP within the transformers library from HuggingFace. The video below uses only CPU.</p>
<iframe width="960" height="520" src="https://www.youtube.com/embed/SKHBvzGCJak" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" title="Physical DonkeyCar"></iframe>
<p>We tried to get this to work on our Jetson Nano, but we soon realized it wasn&#x27;t possible. This was because of the hardware constraints which limited our software environment.</p>
<p>As we previously said, the Jetson Nano supports only Jetpack 4.x and can only support CUDA 10.x. It is also limited to Ubuntu 18.04, which means the Pytorch, Transformer, and Python versions were all limited. Unfortunately, the maximum Transformer version we could use was 4.1x while Pytorch supported 1.10. We were also limited to Python 3.6.
There were various things we had to do in order to even install these dependencies. We had to manually install each from a built wheel, and had to build the transformer library from source.</p>
<p>Since we couldn&#x27;t use BLIP, we decided to partition the vision language model into two parts. A model that utilizes the OAKD-lite on-device VPU to capture embeddings from the image, and a captioner on our Jetson Nano. Unfortunately, we soon realized that in order to do this, we had to train our own model as caption generators does not understand numerical embeddings and as such needed to understand them.</p>
<p>We couldn&#x27;t get this to work, and as such could not advance onwards to the Vector Database and RAG stage.</p>
<h3 class="anchor anchorWithStickyNavbar_FNw8" id="querying">Querying<a href="#querying" class="hash-link" aria-label="Direct link to Querying" title="Direct link to Querying">​</a></h3>
<p>The query phase consists of two core concepts:</p>
<ul>
<li>LLM Prompting</li>
<li>Path Planning</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_FNw8" id="llm-prompting">LLM Prompting<a href="#llm-prompting" class="hash-link" aria-label="Direct link to LLM Prompting" title="Direct link to LLM Prompting">​</a></h4>
<p>The paper compared multiple LLMs, but we decided to use Ollama as it was free. We didn&#x27;t realize that even if it was free, open source, and locally stored, locally stored meant it ran on our computer. Since we tested it on our computers and worked fine (NVIDIA 20/30 series), we thought it would work on our Jetson Nano.
We only realized it after we installed the model onto an 1TB external SSD that it was eating 100% of our CPU and not loading at all. That was because we were using llama3.2, which has 1B and 3B parameters. We switched to gpt2 thinking it wouldn&#x27;t be that far off, but it was significantly worse than any modern LLMs.</p>
<h5 class="anchor anchorWithStickyNavbar_FNw8" id="llama32">Llama3.2<a href="#llama32" class="hash-link" aria-label="Direct link to Llama3.2" title="Direct link to Llama3.2">​</a></h5>
<div style="display:flex;justify-content:center;align-items:center"><img src="https://i.gyazo.com/57c53edf618b97f4f3fe97d6bbf017f6.png" alt="LLAMA3.2"></div>
<h4 class="anchor anchorWithStickyNavbar_FNw8" id="gpt2">GPT2<a href="#gpt2" class="hash-link" aria-label="Direct link to GPT2" title="Direct link to GPT2">​</a></h4>
<div style="display:flex;justify-content:center;align-items:center"><img src="https://i.gyazo.com/eb3bfacd9659ca7073f75661a14cb210.png" alt="LLAMA3.2"></div>
<h4 class="anchor anchorWithStickyNavbar_FNw8" id="path-planning">Path Planning<a href="#path-planning" class="hash-link" aria-label="Direct link to Path Planning" title="Direct link to Path Planning">​</a></h4>
<p>There was no work done for this as we could not get ROS2 SLAM to work. This is as any other path planning there is. Since we have a mapped environment, we can then do path planning.</p>
<h2 class="anchor anchorWithStickyNavbar_FNw8" id="environment">Environment<a href="#environment" class="hash-link" aria-label="Direct link to Environment" title="Direct link to Environment">​</a></h2>
<p>We used:</p>
<ul>
<li>Ubuntu 18.04</li>
<li>CUDA 10.2</li>
<li>Python 3.6</li>
<li>Pytorch 4.10</li>
<li>Transformers 4.1x</li>
<li>ROS2 Dashing,</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_FNw8" id="pivot">Pivot<a href="#pivot" class="hash-link" aria-label="Direct link to Pivot" title="Direct link to Pivot">​</a></h2>
<p>We decided to pivot to another idea, using Human Pose Estimation to using pose to apply various applications such as medical injuries (fall, dislocated, etc), and teaching assistant (posture in class, slouched, etc). This was a great pivot since DepthAI provided the necessary boilerplate and we just needed to create the infrastructure for logic handling.</p>
<div style="display:flex;justify-content:center;align-items:center"><img src="https://i.gyazo.com/1fa6fa911dfcf7235fd174310ceabc8b.png" alt="LLAMA3.2"></div>
<h2 class="anchor anchorWithStickyNavbar_FNw8" id="future-considerations">Future Considerations<a href="#future-considerations" class="hash-link" aria-label="Direct link to Future Considerations" title="Direct link to Future Considerations">​</a></h2>
<p>In the grand scheme of things, this project was too ambitious for our given time constraint as well as our hardware. If we had a chance to redo this, we would ask for a better Jetson which would have solved all software issues.
It is also my personal opinion that I pitched an idea too challenging for my teammates, which meant they did not contribute much.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_aHIs padding--none margin-left--sm"><li class="tag_nwHU"><a class="tag_QDqo tagRegular_RTiO" href="/fall-2024-final-project-team-2/docs/tags/re-m-emb-r">ReMEmbR</a></li><li class="tag_nwHU"><a class="tag_QDqo tagRegular_RTiO" href="/fall-2024-final-project-team-2/docs/tags/research-paper">Research Paper</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"></div><div class="col lastUpdated__GQF"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2024-12-07T08:00:00.000Z">Dec 7, 2024</time></b> by <b>Kevin Shin</b></span></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/fall-2024-final-project-team-2/docs/roboflow"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Roboflow with OAKD-Lite</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/fall-2024-final-project-team-2/docs/human-pose"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Human Pose Estimation</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_IS5x thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#preface" class="table-of-contents__link toc-highlight">Preface</a></li><li><a href="#challenges" class="table-of-contents__link toc-highlight">Challenges</a></li><li><a href="#deliverables" class="table-of-contents__link toc-highlight">Deliverables</a><ul><li><a href="#memory-building" class="table-of-contents__link toc-highlight">Memory Building</a></li><li><a href="#querying" class="table-of-contents__link toc-highlight">Querying</a></li></ul></li><li><a href="#environment" class="table-of-contents__link toc-highlight">Environment</a></li><li><a href="#pivot" class="table-of-contents__link toc-highlight">Pivot</a></li><li><a href="#future-considerations" class="table-of-contents__link toc-highlight">Future Considerations</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Connect</div><ul class="footer__items clean-list"><li class="footer__item"><a href="http://triton-ai.eng.ucsd.edu/" target="_blank" rel="noopener noreferrer" class="footer__link-item">TritonAI<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_T11m"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/UCSD-ECEMAE-148" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_T11m"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title"></div><ul class="footer__items clean-list"></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">
        © 2024 - +2024 Kevin Shin, Built with Docusaurus.
      </div></div></div></footer></div>
</body>
</html>