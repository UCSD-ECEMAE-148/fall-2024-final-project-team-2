"use strict";(self.webpackChunkUCSD_ECEMAE_148=self.webpackChunkUCSD_ECEMAE_148||[]).push([[6790],{6880:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>l});var a=n(4848),o=n(8453);const r={id:"human-pose",title:"Human Pose Estimation",tags:["DepthAI","OAKD-Lite","DepthAI"],sidebar_position:8,last_update:{date:"12/14/2024",author:"Brian Sun"}},s="Human Pose Estimation with OAKD-Lite and DepthAI",i={id:"human-pose/human-pose",title:"Human Pose Estimation",description:"Our final project consisted of a robotic car that uses the OAKD-Lite camera to detect people in different poses and drives up to them to carry out corresponding actions.",source:"@site/docs/human-pose/index.mdx",sourceDirName:"human-pose",slug:"/human-pose/",permalink:"/fall-2024-final-project-team-2/docs/human-pose/",draft:!1,unlisted:!1,tags:[{label:"DepthAI",permalink:"/fall-2024-final-project-team-2/docs/tags/depth-ai"},{label:"OAKD-Lite",permalink:"/fall-2024-final-project-team-2/docs/tags/oakd-lite"}],version:"current",lastUpdatedBy:"Brian Sun",lastUpdatedAt:1734163200,formattedLastUpdatedAt:"Dec 14, 2024",sidebarPosition:8,frontMatter:{id:"human-pose",title:"Human Pose Estimation",tags:["DepthAI","OAKD-Lite","DepthAI"],sidebar_position:8,last_update:{date:"12/14/2024",author:"Brian Sun"}},sidebar:"noteSidebar",previous:{title:"Retrieval-augmented Memory for Embodied Robots",permalink:"/fall-2024-final-project-team-2/docs/remember/"}},d={},l=[];function c(e){const t={a:"a",code:"code",h1:"h1",img:"img",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.h1,{id:"human-pose-estimation-with-oakd-lite-and-depthai",children:"Human Pose Estimation with OAKD-Lite and DepthAI"}),"\n",(0,a.jsx)(t.p,{children:"Our final project consisted of a robotic car that uses the OAKD-Lite camera to detect people in different poses and drives up to them to carry out corresponding actions.\r\nThe goal was to detect and carry out actions for 3 main poses: a person standing up, a person with their head down, and a person with their hand raised.\r\nWe were also able to detect a person sitting up straight, in which case the robot would not do anything."}),"\n",(0,a.jsx)(t.p,{children:"To start, we used the class docker container for our code. Within our docker container, we have to ensure the X11 forwarding is set up. Once set up, we can proceed to give permissions to the OAKD-lite camera.\r\nThis can be done by running these commands within both Jetson Nano and inside our docker container."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:'export OPENBLAS_CORETYPE=ARMV8" >> ~/.bashrc\r\necho \'SUBSYSTEM=="usb", ATTRS{idVendor}=="03e7", MODE="0666"\' | sudo tee /etc/udev/rules.d/80-movidius.rules\n'})}),"\n",(0,a.jsx)(t.p,{children:'The key part of this project were the coordinates for each "landmark" on the human body. Using the DepthAI human pose model for reference, each point in the display window is a landmark that tracks a certain body part.\r\nBy accessing the coordinates for each landmark, we can write code to compare the positions of landmarks against each other and find out if a person is doing a certain pose.'}),"\n",(0,a.jsxs)(t.p,{children:["However, the Luxonis DepthAI model used in Roboflow has landmark points that are inaccessible without rewriting a lot of the code. Instead, we used a GitHub repository called depthai_blazepose.\r\nThis repository uses BlazePose, a pose detection model created by Google researchers and integrates it into the DepthAI library to run on our OAKD-Lite camera.\r\nThe link to the repository can be found here: ",(0,a.jsx)(t.a,{href:"https://github.com/geaxgx/depthai_blazepose",children:"DepthAI BlazePose"})]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"RUN git clone https://github.com/geaxgx/depthai_blazepose.git && \\\r\n    cd depthai_blazepose && \\\r\n    python3 -m pip install -r requirements.txt\n"})}),"\n",(0,a.jsx)(t.p,{children:"Our team ran into certification errors while installing the requirements. If this happens, exit out of the docker container and restart it.\r\nFurthermore, there were issues with installing open3d from the reqirements text file. Manually installing open3d will fix the issue."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"RUN pip install open3d\n"})}),"\n",(0,a.jsxs)(t.p,{children:["Once done, we can run the model within ",(0,a.jsx)(t.code,{children:"/depthai_blazepose/"}),":"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"RUN python3 demo.py\n"})}),"\n",(0,a.jsxs)(t.p,{children:["There are numerous arguments that can be added to the end of demo.py. These are optional, but the two notable ones are ",(0,a.jsx)(t.code,{children:"lm_m ['lite', 'heavy', 'full']"})," and ",(0,a.jsx)(t.code,{children:"-xyz"}),".\r\nThe argument ",(0,a.jsx)(t.code,{children:"lm_m"})," has to be followed with one of the three options and is used to select a lighter or heavier model for better performance on your hardware.\r\nThe argument ",(0,a.jsx)(t.code,{children:"-xyz"})," displays the distance of a person's waist relative to the center of the camera in centimeters."]}),"\n",(0,a.jsx)(t.p,{children:"Note that the y-coordinate of distance and landmarks is negative when above a person's waist and positive when below a person's waist."}),"\n",(0,a.jsx)(t.p,{children:"To display the coordinates of all landmarks, open demo.py in a text editor and navigate to this code block:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"while True:\r\n    # Run blazepose on next frame\r\n    frame, body = tracker.next_frame()\r\n    if frame is None: break\r\n    # Draw 2d skeleton\r\n    frame = renderer.draw(frame, body)\r\n    key = renderer.waitKey(delay=1)\r\n    if key == 27 or key == ord('q'):\r\n        break\r\nrenderer.exit()\r\ntracker.exit()\n"})}),"\n",(0,a.jsx)(t.p,{children:"Add the following lines of code under the first break statement:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"if body:\r\n        landmark_coords = body.landmarks_world\n"})}),"\n",(0,a.jsx)(t.p,{children:"This will create a list called landmark_coords that constantly display the location all landmarks detected by BlazePose. Each element is a list in the form [x, y, z]."}),"\n",(0,a.jsxs)(t.p,{children:["The image below shows all of the landmarks detected by BlazePose. The numbered list corresponds with each landmark's index in ",(0,a.jsx)(t.code,{children:"landmark_coords"}),".\r\n",(0,a.jsx)(t.img,{src:"https://raw.githubusercontent.com/geaxgx/depthai_blazepose/main/img/full_body_landmarks.png",alt:"Landmarks"})]}),"\n",(0,a.jsx)(t.p,{children:"A video showcasing pose detection is down below:"}),"\n",(0,a.jsx)("iframe",{width:"560",height:"315",src:"https://youtube.com/embed/JBTBDDdKwjo",title:"BlazePose Pose Detection",allowfullscreen:!0}),"\n",(0,a.jsx)(t.p,{children:"A video showcasing pose detection with driving functionality is down below:"}),"\n",(0,a.jsx)("iframe",{width:"560",height:"315",src:"https://youtube.com/embed/jgFnrI5N3qA",title:"DepthAI while Driving",allowfullscreen:!0})]})}function h(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>i});var a=n(6540);const o={},r=a.createContext(o);function s(e){const t=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),a.createElement(r.Provider,{value:t},e.children)}}}]);