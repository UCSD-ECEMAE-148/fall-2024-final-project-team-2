"use strict";(self.webpackChunkUCSD_ECEMAE_148=self.webpackChunkUCSD_ECEMAE_148||[]).push([[9228],{2606:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>l,metadata:()=>d,toc:()=>h});var s=n(4848),a=n(8453),i=n(7158),r=n(614);const l={id:"remember",title:"Retrieval-augmented Memory for Embodied Robots",tags:["ReMEmbR","Research Paper"],sidebar_position:7,last_update:{date:"12/7/2024",author:"Kevin Shin"}},o="Retrieval-augmented Memory for Embodied Robots (ReMEmbR)",d={id:"remember/remember",title:"Retrieval-augmented Memory for Embodied Robots",description:"Preface",source:"@site/docs/remember/index.mdx",sourceDirName:"remember",slug:"/remember/",permalink:"/fall-2024-final-project-team-2/docs/remember/",draft:!1,unlisted:!1,tags:[{label:"ReMEmbR",permalink:"/fall-2024-final-project-team-2/docs/tags/re-m-emb-r"},{label:"Research Paper",permalink:"/fall-2024-final-project-team-2/docs/tags/research-paper"}],version:"current",lastUpdatedBy:"Kevin Shin",lastUpdatedAt:1733558400,formattedLastUpdatedAt:"Dec 7, 2024",sidebarPosition:7,frontMatter:{id:"remember",title:"Retrieval-augmented Memory for Embodied Robots",tags:["ReMEmbR","Research Paper"],sidebar_position:7,last_update:{date:"12/7/2024",author:"Kevin Shin"}},sidebar:"noteSidebar",previous:{title:"Roboflow with OAKD-Lite",permalink:"/fall-2024-final-project-team-2/docs/roboflow/"},next:{title:"Human Pose Estimation",permalink:"/fall-2024-final-project-team-2/docs/human-pose/"}},c={},h=[{value:"Preface",id:"preface",level:2},{value:"Challenges",id:"challenges",level:2},{value:"Deliverables",id:"deliverables",level:2},{value:"Memory Building",id:"memory-building",level:3},{value:"SLAM",id:"slam",level:4},{value:"Vision Language Model",id:"vision-language-model",level:4},{value:"Vector Database",id:"vector-database",level:4},{value:"Retrieval Augmented Generation",id:"retrieval-augmented-generation",level:4},{value:"Implementation Challenges",id:"implementation-challenges",level:4},{value:"SLAM",id:"slam-1",level:4},{value:"Vision Language Model",id:"vision-language-model-1",level:5},{value:"Querying",id:"querying",level:3},{value:"LLM Prompting",id:"llm-prompting",level:4},{value:"Llama3.2",id:"llama32",level:5},{value:"GPT2",id:"gpt2",level:4},{value:"Path Planning",id:"path-planning",level:4},{value:"Environment",id:"environment",level:2},{value:"Pivot",id:"pivot",level:2},{value:"Future Considerations",id:"future-considerations",level:2}];function m(e){const t={a:"a",admonition:"admonition",annotation:"annotation",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",li:"li",math:"math",mi:"mi",mo:"mo",mrow:"mrow",msup:"msup",p:"p",semantics:"semantics",span:"span",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.h1,{id:"retrieval-augmented-memory-for-embodied-robots-remembr",children:"Retrieval-augmented Memory for Embodied Robots (ReMEmbR)"}),"\n",(0,s.jsx)(t.h2,{id:"preface",children:"Preface"}),"\n",(0,s.jsxs)(t.p,{children:["The goal of the final project was to replicate a ",(0,s.jsx)(t.a,{href:"https://nvidia-ai-iot.github.io/remembr/",children:"research paper"})," that allowed robots to establish memory and reason through the memory. It could go back to a given position it has already been to with a prompt.\r\nFor example, if asked where the nearest restroom was, it would be able to path plan to the area where it has seen a restroom."]}),"\n",(0,s.jsx)("iframe",{width:"960",height:"520",src:"https://nvidia-ai-iot.github.io/remembr/static/video/demo.mp4",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowfullscreen:!0,title:"Physical DonkeyCar"}),"\n",(0,s.jsx)(t.admonition,{type:"note",children:(0,s.jsx)(t.p,{children:"This video is taken from the original research paper."})}),"\n",(0,s.jsx)(t.h2,{id:"challenges",children:"Challenges"}),"\n",(0,s.jsxs)(t.p,{children:["The paper used the ",(0,s.jsx)(t.a,{href:"https://robotics.segway.com/nova-carter/",children:"Segway Nova Carter"})," which is built on top of the Jetson AGX Orin.\r\nThis is an immense difference with our Jetson Nano, almost like comparing a jet to a bicycle. As such, our goal was to replicate this on a constrained and limited hardware platform."]}),"\n",(0,s.jsx)(t.p,{children:"Below is a note of the difference in capability."}),"\n",(0,s.jsxs)(i.A,{children:[(0,s.jsx)(r.A,{value:"CPU",label:"CPU",children:(0,s.jsxs)("table",{children:[(0,s.jsx)("thead",{children:(0,s.jsxs)("tr",{children:[(0,s.jsx)("th",{children:"Aspect"}),(0,s.jsx)("th",{children:"Jetson AGX Orin"}),(0,s.jsx)("th",{children:"Jetson Nano"})]})}),(0,s.jsxs)("tbody",{children:[(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Processor"}),(0,s.jsx)("td",{children:"12-core ARM Cortex-A78AE"}),(0,s.jsx)("td",{children:"Quad-core ARM Cortex-A57"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Clock Speed"}),(0,s.jsx)("td",{children:"~2.2 GHz"}),(0,s.jsx)("td",{children:"~1.43 GHz"})]})]})]})}),(0,s.jsx)(r.A,{value:"GPU",label:"GPU",children:(0,s.jsxs)("table",{children:[(0,s.jsx)("thead",{children:(0,s.jsxs)("tr",{children:[(0,s.jsx)("th",{children:"Aspect"}),(0,s.jsx)("th",{children:"Jetson AGX Orin"}),(0,s.jsx)("th",{children:"Jetson Nano"})]})}),(0,s.jsxs)("tbody",{children:[(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Architecture"}),(0,s.jsx)("td",{children:"NVIDIA Ampere"}),(0,s.jsx)("td",{children:"NVIDIA Maxwell"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"CUDA Cores"}),(0,s.jsx)("td",{children:"2048 CUDA cores"}),(0,s.jsx)("td",{children:"128 CUDA cores"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Tensor Cores"}),(0,s.jsx)("td",{children:"64 Tensor Cores"}),(0,s.jsx)("td",{children:"None"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Performance (Trillions of Operations Per Second)"}),(0,s.jsx)("td",{children:"275 TOPS"}),(0,s.jsx)("td",{children:"~0.5 TOPS"})]})]})]})}),(0,s.jsx)(r.A,{value:"RAM",label:"RAM",children:(0,s.jsxs)("table",{children:[(0,s.jsx)("thead",{children:(0,s.jsxs)("tr",{children:[(0,s.jsx)("th",{children:"Aspect"}),(0,s.jsx)("th",{children:"Jetson AGX Orin"}),(0,s.jsx)("th",{children:"Jetson Nano"})]})}),(0,s.jsxs)("tbody",{children:[(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Type"}),(0,s.jsx)("td",{children:"64GB LPDDR5"}),(0,s.jsx)("td",{children:"4GB LPDDR4"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Speed"}),(0,s.jsx)("td",{children:"High bandwidth (256-bit)"}),(0,s.jsx)("td",{children:"Lower bandwidth (64-bit)"})]})]})]})}),(0,s.jsx)(r.A,{value:"Storage",label:"Storage",children:(0,s.jsxs)("table",{children:[(0,s.jsx)("thead",{children:(0,s.jsxs)("tr",{children:[(0,s.jsx)("th",{children:"Aspect"}),(0,s.jsx)("th",{children:"Jetson AGX Orin"}),(0,s.jsx)("th",{children:"Jetson Nano"})]})}),(0,s.jsxs)("tbody",{children:[(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Default Storage"}),(0,s.jsx)("td",{children:"2TB NVMe SSD"}),(0,s.jsx)("td",{children:"microSD card"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Interface"}),(0,s.jsx)("td",{children:"PCIe Gen 4"}),(0,s.jsx)("td",{children:"microSD interface"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Speed"}),(0,s.jsx)("td",{children:"High-speed NVMe"}),(0,s.jsx)("td",{children:"Slower microSD"})]})]})]})}),(0,s.jsx)(r.A,{value:"I/O",label:"I/O",children:(0,s.jsxs)("table",{children:[(0,s.jsx)("thead",{children:(0,s.jsxs)("tr",{children:[(0,s.jsx)("th",{children:"Aspect"}),(0,s.jsx)("th",{children:"Jetson AGX Orin"}),(0,s.jsx)("th",{children:"Jetson Nano"})]})}),(0,s.jsxs)("tbody",{children:[(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Networking"}),(0,s.jsx)("td",{children:"10GbE PCIe (optional)"}),(0,s.jsx)("td",{children:"1GbE Ethernet"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"USB"}),(0,s.jsx)("td",{children:"USB 3.1, USB 3.2 Gen 2"}),(0,s.jsx)("td",{children:"USB 3.0"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"Camera Interfaces"}),(0,s.jsx)("td",{children:"16x MIPI CSI"}),(0,s.jsx)("td",{children:"2x MIPI CSI"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"GPIO"}),(0,s.jsx)("td",{children:"Rich GPIO, AI-optimized"}),(0,s.jsx)("td",{children:"Standard GPIO"})]})]})]})})]}),"\n",(0,s.jsx)(t.p,{children:"It's also important to note that the Jetson Nano is limited to Ubuntu 18.04. With some tweaking, it can be flashed to Ubuntu 20.04, but the underlying driver software that supports the GPU is limited to 10.x or Jetpack 4.x. As such, without even considering hardware limitations, our software can only rely on outdated unoptimized GPU libraries."}),"\n",(0,s.jsx)(t.p,{children:"The Jetson AGX Orin on the other hand defaults on Ubuntu 20.04 and uses the Jetpack 5.x as the base minimum. This means that out of the box, Jetson AGX Orin already has all the optimized software libraries for GPU-acceleration."}),"\n",(0,s.jsx)(t.h2,{id:"deliverables",children:"Deliverables"}),"\n",(0,s.jsx)(t.p,{children:"The bare minimum for this paper is to replicate the behavior, at least to some degree. This meant that we had to implement the two core functionalities, which were memory building and query. Below is an image that represents the two phases."}),"\n",(0,s.jsx)("div",{style:{display:"flex",justifyContent:"center",alignItems:"center"},children:(0,s.jsx)("img",{src:"https://i.gyazo.com/3a152edff226769acb253fab8e4d1610.png",alt:"Img"})}),"\n",(0,s.jsx)(t.p,{children:"This means we can partition the deliverable into two."}),"\n",(0,s.jsx)(t.h3,{id:"memory-building",children:"Memory Building"}),"\n",(0,s.jsx)(t.p,{children:"The underlying technology for building memory consists of three core tools."}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"3D/2D SLAM Mapping"}),"\n",(0,s.jsx)(t.li,{children:"Vision Language model"}),"\n",(0,s.jsx)(t.li,{children:"Vector Database"}),"\n",(0,s.jsx)(t.li,{children:"Retrieval Augmented Generation"}),"\n"]}),"\n",(0,s.jsx)(t.h4,{id:"slam",children:"SLAM"}),"\n",(0,s.jsx)(t.p,{children:"SLAM is simultaneous localization and mapping, which maps the environment around us using LIDAR (light)."}),"\n",(0,s.jsx)(t.h4,{id:"vision-language-model",children:"Vision Language Model"}),"\n",(0,s.jsx)(t.p,{children:"A Vision Language Model is a deep learning computer vision model that generates additional information alongside the captured scenery. This means that instead of the conventional object detection or segmentation models, each image or a batch of images can have captions which provides natural language information."}),"\n",(0,s.jsx)(t.h4,{id:"vector-database",children:"Vector Database"}),"\n",(0,s.jsxs)(t.p,{children:["A vector database is simply a database that uses the inherent properties of vectors to store data. A vector is an element that belongs to ",(0,s.jsxs)(t.span,{className:"katex",children:[(0,s.jsx)(t.span,{className:"katex-mathml",children:(0,s.jsx)(t.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(t.semantics,{children:[(0,s.jsx)(t.mrow,{children:(0,s.jsxs)(t.msup,{children:[(0,s.jsx)(t.mi,{children:"R"}),(0,s.jsx)(t.mi,{children:"n"})]})}),(0,s.jsx)(t.annotation,{encoding:"application/x-tex",children:"R^{n}"})]})})}),(0,s.jsx)(t.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(t.span,{className:"base",children:[(0,s.jsx)(t.span,{className:"strut",style:{height:"0.6833em"}}),(0,s.jsxs)(t.span,{className:"mord",children:[(0,s.jsx)(t.span,{className:"mord mathnormal",style:{marginRight:"0.00773em"},children:"R"}),(0,s.jsx)(t.span,{className:"msupsub",children:(0,s.jsx)(t.span,{className:"vlist-t",children:(0,s.jsx)(t.span,{className:"vlist-r",children:(0,s.jsx)(t.span,{className:"vlist",style:{height:"0.6644em"},children:(0,s.jsxs)(t.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,s.jsx)(t.span,{className:"pstrut",style:{height:"2.7em"}}),(0,s.jsx)(t.span,{className:"sizing reset-size6 size3 mtight",children:(0,s.jsx)(t.span,{className:"mord mtight",children:(0,s.jsx)(t.span,{className:"mord mathnormal mtight",children:"n"})})})]})})})})})]})]})})]}),". This is no different than an image that is ",(0,s.jsxs)(t.span,{className:"katex",children:[(0,s.jsx)(t.span,{className:"katex-mathml",children:(0,s.jsx)(t.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(t.semantics,{children:[(0,s.jsx)(t.mrow,{children:(0,s.jsxs)(t.msup,{children:[(0,s.jsx)(t.mi,{mathvariant:"double-struck",children:"R"}),(0,s.jsxs)(t.mrow,{children:[(0,s.jsx)(t.mi,{children:"n"}),(0,s.jsx)(t.mo,{children:"\xd7"}),(0,s.jsx)(t.mi,{children:"m"})]})]})}),(0,s.jsx)(t.annotation,{encoding:"application/x-tex",children:"\\mathbb{R}^{n\\times m}"})]})})}),(0,s.jsx)(t.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(t.span,{className:"base",children:[(0,s.jsx)(t.span,{className:"strut",style:{height:"0.7713em"}}),(0,s.jsxs)(t.span,{className:"mord",children:[(0,s.jsx)(t.span,{className:"mord mathbb",children:"R"}),(0,s.jsx)(t.span,{className:"msupsub",children:(0,s.jsx)(t.span,{className:"vlist-t",children:(0,s.jsx)(t.span,{className:"vlist-r",children:(0,s.jsx)(t.span,{className:"vlist",style:{height:"0.7713em"},children:(0,s.jsxs)(t.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,s.jsx)(t.span,{className:"pstrut",style:{height:"2.7em"}}),(0,s.jsx)(t.span,{className:"sizing reset-size6 size3 mtight",children:(0,s.jsxs)(t.span,{className:"mord mtight",children:[(0,s.jsx)(t.span,{className:"mord mathnormal mtight",children:"n"}),(0,s.jsx)(t.span,{className:"mbin mtight",children:"\xd7"}),(0,s.jsx)(t.span,{className:"mord mathnormal mtight",children:"m"})]})})]})})})})})]})]})})]})," and we partition the image into ",(0,s.jsxs)(t.span,{className:"katex",children:[(0,s.jsx)(t.span,{className:"katex-mathml",children:(0,s.jsx)(t.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(t.semantics,{children:[(0,s.jsx)(t.mrow,{children:(0,s.jsx)(t.mi,{children:"m"})}),(0,s.jsx)(t.annotation,{encoding:"application/x-tex",children:"m"})]})})}),(0,s.jsx)(t.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(t.span,{className:"base",children:[(0,s.jsx)(t.span,{className:"strut",style:{height:"0.4306em"}}),(0,s.jsx)(t.span,{className:"mord mathnormal",children:"m"})]})})]})," different column vectors of ",(0,s.jsxs)(t.span,{className:"katex",children:[(0,s.jsx)(t.span,{className:"katex-mathml",children:(0,s.jsx)(t.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,s.jsxs)(t.semantics,{children:[(0,s.jsx)(t.mrow,{children:(0,s.jsxs)(t.msup,{children:[(0,s.jsx)(t.mi,{children:"R"}),(0,s.jsx)(t.mi,{children:"n"})]})}),(0,s.jsx)(t.annotation,{encoding:"application/x-tex",children:"R^{n}"})]})})}),(0,s.jsx)(t.span,{className:"katex-html","aria-hidden":"true",children:(0,s.jsxs)(t.span,{className:"base",children:[(0,s.jsx)(t.span,{className:"strut",style:{height:"0.6833em"}}),(0,s.jsxs)(t.span,{className:"mord",children:[(0,s.jsx)(t.span,{className:"mord mathnormal",style:{marginRight:"0.00773em"},children:"R"}),(0,s.jsx)(t.span,{className:"msupsub",children:(0,s.jsx)(t.span,{className:"vlist-t",children:(0,s.jsx)(t.span,{className:"vlist-r",children:(0,s.jsx)(t.span,{className:"vlist",style:{height:"0.6644em"},children:(0,s.jsxs)(t.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,s.jsx)(t.span,{className:"pstrut",style:{height:"2.7em"}}),(0,s.jsx)(t.span,{className:"sizing reset-size6 size3 mtight",children:(0,s.jsx)(t.span,{className:"mord mtight",children:(0,s.jsx)(t.span,{className:"mord mathnormal mtight",children:"n"})})})]})})})})})]})]})})]}),". This is useful because we can now apply mathematics for similarity. Before we do so though, we have to encode any non-numerical information into numerics. This can be done by any method."]}),"\n",(0,s.jsx)(t.h4,{id:"retrieval-augmented-generation",children:"Retrieval Augmented Generation"}),"\n",(0,s.jsx)(t.p,{children:"Retrieval Augmented Generation allows us to find which vectors are the most similar so that we can deliver the one that's relevant. In this context, given one vector representing a command, we can apply RAG techniques such as cosine-similarity to see which vector within our memory database is the closest. Since we have caption information for our vectors, both query and memory vectors live in the same embedding space."}),"\n",(0,s.jsx)(t.h4,{id:"implementation-challenges",children:"Implementation Challenges"}),"\n",(0,s.jsx)(t.h4,{id:"slam-1",children:"SLAM"}),"\n",(0,s.jsx)(t.p,{children:"The original paper used 3D LIDAR, but since we had an OAKD-Lite that provided a variety of features such as depth and distance to said object, we only needed 2D LIDAR. Unfortunately, we could not set up SLAM because of environmental incompatibility. Since our Jetson Nano supported Ubuntu 18.04, and the ROS2 framework for DepthAI required Ubuntu versions 20.04 and newer, we had to resort to not using it. This meant that we had to build our own internal boilerplate for figuring out the height."}),"\n",(0,s.jsx)(t.h5,{id:"vision-language-model-1",children:"Vision Language Model"}),"\n",(0,s.jsx)(t.p,{children:"Since we knew the limitations of our hardware, we had to choose another VLM other than the one used in the research paper which was VILA. We first decided to create an MVP to see if it was possible on our personal computer, and saw that it indeed was possible.\r\nBelow is us utilizing BLIP within the transformers library from HuggingFace. The video below uses only CPU."}),"\n",(0,s.jsx)("iframe",{width:"960",height:"520",src:"https://www.youtube.com/embed/SKHBvzGCJak",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowfullscreen:!0,title:"Physical DonkeyCar"}),"\n",(0,s.jsx)(t.p,{children:"We tried to get this to work on our Jetson Nano, but we soon realized it wasn't possible. This was because of the hardware constraints which limited our software environment."}),"\n",(0,s.jsx)(t.p,{children:"As we previously said, the Jetson Nano supports only Jetpack 4.x and can only support CUDA 10.x. It is also limited to Ubuntu 18.04, which means the Pytorch, Transformer, and Python versions were all limited. Unfortunately, the maximum Transformer version we could use was 4.1x while Pytorch supported 1.10. We were also limited to Python 3.6.\r\nThere were various things we had to do in order to even install these dependencies. We had to manually install each from a built wheel, and had to build the transformer library from source."}),"\n",(0,s.jsx)(t.p,{children:"Since we couldn't use BLIP, we decided to partition the vision language model into two parts. A model that utilizes the OAKD-lite on-device VPU to capture embeddings from the image, and a captioner on our Jetson Nano. Unfortunately, we soon realized that in order to do this, we had to train our own model as caption generators does not understand numerical embeddings and as such needed to understand them."}),"\n",(0,s.jsx)(t.p,{children:"We couldn't get this to work, and as such could not advance onwards to the Vector Database and RAG stage."}),"\n",(0,s.jsx)(t.h3,{id:"querying",children:"Querying"}),"\n",(0,s.jsx)(t.p,{children:"The query phase consists of two core concepts:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"LLM Prompting"}),"\n",(0,s.jsx)(t.li,{children:"Path Planning"}),"\n"]}),"\n",(0,s.jsx)(t.h4,{id:"llm-prompting",children:"LLM Prompting"}),"\n",(0,s.jsx)(t.p,{children:"The paper compared multiple LLMs, but we decided to use Ollama as it was free. We didn't realize that even if it was free, open source, and locally stored, locally stored meant it ran on our computer. Since we tested it on our computers and worked fine (NVIDIA 20/30 series), we thought it would work on our Jetson Nano.\r\nWe only realized it after we installed the model onto an 1TB external SSD that it was eating 100% of our CPU and not loading at all. That was because we were using llama3.2, which has 1B and 3B parameters. We switched to gpt2 thinking it wouldn't be that far off, but it was significantly worse than any modern LLMs."}),"\n",(0,s.jsx)(t.h5,{id:"llama32",children:"Llama3.2"}),"\n",(0,s.jsx)("div",{style:{display:"flex",justifyContent:"center",alignItems:"center"},children:(0,s.jsx)("img",{src:"https://i.gyazo.com/57c53edf618b97f4f3fe97d6bbf017f6.png",alt:"LLAMA3.2"})}),"\n",(0,s.jsx)(t.h4,{id:"gpt2",children:"GPT2"}),"\n",(0,s.jsx)("div",{style:{display:"flex",justifyContent:"center",alignItems:"center"},children:(0,s.jsx)("img",{src:"https://i.gyazo.com/eb3bfacd9659ca7073f75661a14cb210.png",alt:"LLAMA3.2"})}),"\n",(0,s.jsx)(t.h4,{id:"path-planning",children:"Path Planning"}),"\n",(0,s.jsx)(t.p,{children:"There was no work done for this as we could not get ROS2 SLAM to work. This is as any other path planning there is. Since we have a mapped environment, we can then do path planning."}),"\n",(0,s.jsx)(t.h2,{id:"environment",children:"Environment"}),"\n",(0,s.jsx)(t.p,{children:"We used:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Ubuntu 18.04"}),"\n",(0,s.jsx)(t.li,{children:"CUDA 10.2"}),"\n",(0,s.jsx)(t.li,{children:"Python 3.6"}),"\n",(0,s.jsx)(t.li,{children:"Pytorch 4.10"}),"\n",(0,s.jsx)(t.li,{children:"Transformers 4.1x"}),"\n",(0,s.jsx)(t.li,{children:"ROS2 Dashing,"}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"pivot",children:"Pivot"}),"\n",(0,s.jsx)(t.p,{children:"We decided to pivot to another idea, using Human Pose Estimation to using pose to apply various applications such as medical injuries (fall, dislocated, etc), and teaching assistant (posture in class, slouched, etc). This was a great pivot since DepthAI provided the necessary boilerplate and we just needed to create the infrastructure for logic handling."}),"\n",(0,s.jsx)("div",{style:{display:"flex",justifyContent:"center",alignItems:"center"},children:(0,s.jsx)("img",{src:"https://i.gyazo.com/1fa6fa911dfcf7235fd174310ceabc8b.png",alt:"LLAMA3.2"})}),"\n",(0,s.jsx)(t.h2,{id:"future-considerations",children:"Future Considerations"}),"\n",(0,s.jsx)(t.p,{children:"In the grand scheme of things, this project was too ambitious for our given time constraint as well as our hardware. If we had a chance to redo this, we would ask for a better Jetson which would have solved all software issues.\r\nIt is also my personal opinion that I pitched an idea too challenging for my teammates, which meant they did not contribute much."})]})}function u(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(m,{...e})}):m(e)}},614:(e,t,n)=>{n.d(t,{A:()=>r});n(6540);var s=n(4164);const a={tabItem:"tabItem__kUE"};var i=n(4848);function r(e){let{children:t,hidden:n,className:r}=e;return(0,i.jsx)("div",{role:"tabpanel",className:(0,s.A)(a.tabItem,r),hidden:n,children:t})}},7158:(e,t,n)=>{n.d(t,{A:()=>y});var s=n(6540),a=n(4164),i=n(4245),r=n(6347),l=n(6494),o=n(2814),d=n(5167),c=n(1269);function h(e){return s.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,s.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function m(e){const{values:t,children:n}=e;return(0,s.useMemo)((()=>{const e=t??function(e){return h(e).map((e=>{let{props:{value:t,label:n,attributes:s,default:a}}=e;return{value:t,label:n,attributes:s,default:a}}))}(n);return function(e){const t=(0,d.X)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,n])}function u(e){let{value:t,tabValues:n}=e;return n.some((e=>e.value===t))}function p(e){let{queryString:t=!1,groupId:n}=e;const a=(0,r.W6)(),i=function(e){let{queryString:t=!1,groupId:n}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:t,groupId:n});return[(0,o.aZ)(i),(0,s.useCallback)((e=>{if(!i)return;const t=new URLSearchParams(a.location.search);t.set(i,e),a.replace({...a.location,search:t.toString()})}),[i,a])]}function x(e){const{defaultValue:t,queryString:n=!1,groupId:a}=e,i=m(e),[r,o]=(0,s.useState)((()=>function(e){let{defaultValue:t,tabValues:n}=e;if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!u({value:t,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const s=n.find((e=>e.default))??n[0];if(!s)throw new Error("Unexpected error: 0 tabValues");return s.value}({defaultValue:t,tabValues:i}))),[d,h]=p({queryString:n,groupId:a}),[x,j]=function(e){let{groupId:t}=e;const n=function(e){return e?`docusaurus.tab.${e}`:null}(t),[a,i]=(0,c.Dv)(n);return[a,(0,s.useCallback)((e=>{n&&i.set(e)}),[n,i])]}({groupId:a}),g=(()=>{const e=d??x;return u({value:e,tabValues:i})?e:null})();(0,l.A)((()=>{g&&o(g)}),[g]);return{selectedValue:r,selectValue:(0,s.useCallback)((e=>{if(!u({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);o(e),h(e),j(e)}),[h,j,i]),tabValues:i}}var j=n(1062);const g={tabList:"tabList_fbd4",tabItem:"tabItem_v5XY"};var b=n(4848);function f(e){let{className:t,block:n,selectedValue:s,selectValue:r,tabValues:l}=e;const o=[],{blockElementScrollPositionUntilNextRender:d}=(0,i.a_)(),c=e=>{const t=e.currentTarget,n=o.indexOf(t),a=l[n].value;a!==s&&(d(t),r(a))},h=e=>{let t=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const n=o.indexOf(e.currentTarget)+1;t=o[n]??o[0];break}case"ArrowLeft":{const n=o.indexOf(e.currentTarget)-1;t=o[n]??o[o.length-1];break}}t?.focus()};return(0,b.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":n},t),children:l.map((e=>{let{value:t,label:n,attributes:i}=e;return(0,b.jsx)("li",{role:"tab",tabIndex:s===t?0:-1,"aria-selected":s===t,ref:e=>o.push(e),onKeyDown:h,onClick:c,...i,className:(0,a.A)("tabs__item",g.tabItem,i?.className,{"tabs__item--active":s===t}),children:n??t},t)}))})}function v(e){let{lazy:t,children:n,selectedValue:a}=e;const i=(Array.isArray(n)?n:[n]).filter(Boolean);if(t){const e=i.find((e=>e.props.value===a));return e?(0,s.cloneElement)(e,{className:"margin-top--md"}):null}return(0,b.jsx)("div",{className:"margin-top--md",children:i.map(((e,t)=>(0,s.cloneElement)(e,{key:t,hidden:e.props.value!==a})))})}function w(e){const t=x(e);return(0,b.jsxs)("div",{className:(0,a.A)("tabs-container",g.tabList),children:[(0,b.jsx)(f,{...e,...t}),(0,b.jsx)(v,{...e,...t})]})}function y(e){const t=(0,j.A)();return(0,b.jsx)(w,{...e,children:h(e.children)},String(t))}},8453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>l});var s=n(6540);const a={},i=s.createContext(a);function r(e){const t=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(i.Provider,{value:t},e.children)}}}]);